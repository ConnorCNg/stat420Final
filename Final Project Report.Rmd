0p-ouh---
title: 'FIFA 19 Player Data Analysis'
author: "STAT 420, Summer 2020, Shuo Yan, Chutong Xiao, Connor Ng"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

```{r}
birthday = 19980327
set.seed(birthday)
```

## Introduction

Everyone of us can list our favorite football players; however, we as amateur audiences seldom know why some football players are so famous and why their transfer values are so high. Hopefully, through this study we will be able to understand the standard of player evaluation. Are certain traits in football players more beneficial than others? Is pure physical ability enough to be thought to be as good as Lionel Messi or Cristiano Ronaldo or is there something more?

In this study, we will use the `FIFA_19.csv` dataset to explore the relationship between the overall ratings and other attributes of the players. We aim the find the best model that will help us to determine the rating of a palyer based on existing data.

The variables in the dataset are:

- `ID` 
- `Name` - Fullname of the football player
- `Age` - in years
- `Photo` - Link to the profile photo 
- `Nationality` 
- `Flag` - Link to the national flag photo 
- `Overall` - The overall rating, integer
- `Potential` - The potential rating, integer
- `Club`
- `Club.logo` - Link to the Logo image

- `Value` - in M EUR
- `Wage` - in K EUR
- `Release.Cause` - M EUR

- `Special` - 
- `Preferred.Foot` - right or left
- `International.Reputation` - 
- `Weak.Foot` - 
- `Skill.Moves` -
- `Work.Rate` - 
- `Body.Type` - Lean, normal or stocky
- `Real.Face` - Whether the player has a real face (Yes or No)
- `Position` 
- `Jersey.Number` - Integer that identify the position
- `Joined` - The year when the player joined FIFA
- `Contract.Valid.Until` - Contract end-dates in years
- `Height` - in feet
- `Weight` - in lbs
- `LS` - Left Striker
- `ST` - Striker
- `RS` - Right Striker
- `LW` - Left Wing
- `RW` - Right Wing
- `LF` - Left Forward
- `CF` - Centre Forward
- `RF` - Right Forward
- `LAM` - Left Attacking Midfielder
- `CAM` - Central Attacking Midfielder
- `RAM` - Right Attacking Midfielder
- `LM` - Left Midfielder
- `RM` - Right Midfielder
- `LDM` - Left Defensive Midfielder
- `CDM` - Central Defensive Midfielder
- `RDM` - Right Defensive Midfielder
- `LB` - Left Back (##to delete## notice: we do not have right back(RB) in the dataset)
- `LCM` - Left Centre Midfielder
- `CM` - Central Midfielder
- `RCM` - Right Centre Midfielder
- `LWB` - Left Wing Back
- `RWB` - Right Wing Back

- Attacking: `Crossing`, `Finishing`, `HeadingAccuracy`, `ShortPassing`, `Volleys`

- Skill: `Dribbling`, `Curve`, `FKAccuracy`, `LongPassing`, `BallControl`

- Movement: `Acceleration`, `SprintSpeed`, `Agility`, `Reactions`, `Balance`

- Power: `ShotPower`, `Jumping`, `Stamina`, `Strength`, `LongShots`

- Mentality: `Aggression`, `Interceptions`, `Positioning`, `Vision`, `Penalties`, `Composure`                 
- Defending: `Marking`, `StandingTackle`, `SlidingTackle`

- GoalKeeping: `GKDiving`, `GKHandling`, `GKKicking`, `GKPositioning`,`GKReflexes`






## Methods

### Data Preprocessing

```{r evaluate = FALSE, warning = FALSE}
raw_data = read.csv("/Users/shuoyan/Documents/stat420/finalProject/fifa18-all-player-statistics-master/2019/data.csv")

library(dplyr)
#remove obviously unuseful variables
file = select(raw_data,-c(X,ID, Photo, Flag, Club.Logo, Nationality, Club, Real.Face, Jersey.Number, Loaned.From, Joined, Contract.Valid.Until))

#convert columns LS to RB to numeric values
# take the values with plus sign and without plus sign
position_raw = file[, 17:42]
original = matrix(0, nrow(position_raw), ncol(position_raw))
after = matrix(0, nrow(position_raw), ncol(position_raw)) #######THIS IS LS TO RB WITH PLUS SIGN!!!
for(i in 1:nrow(position_raw)){
  for(j in 1:ncol(position_raw)){
    str = as.character(position_raw[i, j])
    if(str == ""){
      original[i, j] = NA
      after[i, j] = NA
    } else{
      str = strsplit(str, split = "+")[[1]]
      original[i,j] = as.numeric(paste(str[1], str[2], sep = ""))
      after[i,j] = original[i] + as.numeric(str[4])
    }
  }
}

file[, 17:42] = original
file = na.omit(file)

#convert Value into numeric
file$Value = as.character(file$Value)
#remove euro sign and million
temp = substring(strsplit(file$Value, split = "M"), 2) ##########NOTE: this value is in Million
file$Value = as.numeric(temp)
#remove NA
anyNA(file$Value)
if(anyNA(file$Value)){
  file = na.omit(file)
}

#convert wage into numeric
file$Wage = as.character(file$Wage)
#remove euto sign and K 
temp = substring(strsplit(file$Wage, split = "K"), 2) ###########NOTE: this value is in K
file$Wage = as.numeric(temp)
#remove NA
anyNA(file$Wage)


#convert release clause into numeric
file$Release.Clause = as.character(file$Release.Clause)
#remove euro sign and M
temp = substring(strsplit(file$Release.Clause, split = "M"), 2) #########NOTE: this value is in M
file$Release.Clause = as.numeric(temp)
#remove NA
if(anyNA(file$Release.Clause)){
  file = na.omit(file)
}

#convert weight from factor to numeric
#blank is missing data
file = subset(file, class(file$Weight) != as.factor(""))
file$Weight = substr(file$Weight, 1, 3)

#convert height to numeric
#blank is missing data
file = subset(file, class(file$Height) != as.factor(""))
file$Height = as.numeric(gsub("\'", ".", file$Height)) * 0.3048 ########Note: in meters

#convert weight to numeric
file$Weight = as.numeric(file$Weight)

#check if columns are their expected types and remove NA

data.frame(colnames(file))
numeric_index = c(2, 3, 4, 5, 6, 7, 9, 10, 11, 15:77)
factor_index = c(1, 8, 6, 12:14)
for(i in numeric_index){
  #print(class(file[, i]))
  file[,i] = na.omit(file[,i])
}

for(i in factor_index){
  #print(class(file[,i]))
  file = subset(file, class(file[, i])!= as.factor(""))
}

file = na.omit(file)
```


### Data Analysis
#### Check for Collinearity of Numeric parameters

- collinearity of physical scorting and Overall Score

Because a full model crashes rstudio, we first analyze some very important parameters - the scoring of player's physical attributes.

```{r}
selection_model = lm(Overall ~ Age + Height + Weight + Weak.Foot + Skill.Moves + Crossing + Finishing + HeadingAccuracy + ShortPassing + Volleys + Dribbling + Curve + FKAccuracy + LongPassing + BallControl + Acceleration + SprintSpeed + Agility + Reactions + Balance + ShotPower + Jumping + Stamina + Strength + LongShots + Aggression + Interceptions + Positioning + Vision + Penalties + Composure +  Marking + StandingTackle + SlidingTackle + GKDiving + GKHandling + GKKicking + GKPositioning + GKReflexes, data = file)

library(faraway)

vif(selection_model)
```
To ensure that the final model doesn't have collinearity issues, we would select parameters with vif less than 5. Therefore we select parameters Age, Height, Weight, Weak.Foot, Skill.Moves, Crossing, HeadingAccuracy, ShortPassing, Volleys, Curve, FKAccuracy, LongPassing, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, Aggression, Vision, Penalties, Composure, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes.

- correlation between Overall score and position scores

```{r}
selection_model = lm(Overall ~ ., data = file[, c(3, 17:42)])
vif(selection_model)
```

We see that all of the parameters have large vifs. To avoid collinearity issues, we would use none of them.

Therefore, we would use the above parameters and select remaining categorical parameters to fit models.

#### Check for Influential Points

```{r}
check_influential_model = lm(Overall ~ Age + Height + Weight + Weak.Foot + Skill.Moves + Crossing + HeadingAccuracy + ShortPassing + Volleys + Curve + FKAccuracy + LongPassing + Agility + Reactions + Balance + ShotPower + Jumping + Stamina + Strength  + Aggression + Vision + Penalties + Composure + GKDiving + GKHandling + GKKicking + GKPositioning + GKReflexes, data = file)

which(cooks.distance(check_influential_model) > 4 / length(cooks.distance(check_influential_model)))
```

We wouldn't consider removing any influential points because there are many of them. Also consider that talents vary, it's unreasonable to remove them.

We have finished analyzing data and variables. Now we can fit models!

### Model Fitting

-helper function
```{r}
calc_loocv_rmse = function(model){
    sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

We first split train and test data. 

```{r}
train_index = sample(nrow(file), 3500)
train_data = file[train_index, ]
test_data = file[-train_index,]
```

Because there are way too many parameters, we first try BIC for model selection

#### model 1

```{r}
temp_model_1 = lm(Overall ~ Age + Height + Weight + Weak.Foot + Skill.Moves + Crossing + HeadingAccuracy + ShortPassing + Volleys + Curve + FKAccuracy + LongPassing + Agility + Reactions + Balance + ShotPower + Jumping + Stamina + Strength  + Aggression + Vision + Penalties + Composure + GKDiving + GKHandling + GKKicking + GKPositioning + GKReflexes + Preferred.Foot + Work.Rate + Body.Type + Position, data = test_data)

model1 = step(temp_model_1, k = log(length(resid(temp_model_1))), trace = 0)

names(coef(model1))
```

### Model Diagnostic
#### Model 1

- test for model assumptions

```{r}
par(mfrow = c(1, 2))
plot(fitted(model1), resid(model1), col = "grey")
abline(h = 0, col = "orange", lwd = 2)

qqnorm(resid(model1), col = "grey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)
```

```{r}
library("lmtest")
bptest(model1)

shapiro.test(resid(model1))
```

As we can see from the Fitted vs. Residuals Plot, the data points clearly have a pattern. It is verified by the BP test where p-value is extremely small. From Normal Q-Q Plot, we see that data points are not obviously off. The Shapiro-Wilk test result is also not significantly small. We wouldn't consider rejecting the normality assumption.

- test for loocv-rmse
```{r}
calc_loocv_rmse(model1)
```

- test for $R^2$
```{r}
summary(model1)$adj.r.squared
```

The LOOCV-RMSE value is very small, which is good. The adjusted $R^2$ value is also reasonably good (high). However, the breach of the non-constant variance assumption must be addressed.

#### Model 2
```{r}
temp_model_2 = lm(Overall ~ Age + Height + Weight + Weak.Foot + Skill.Moves + Crossing + HeadingAccuracy + ShortPassing + Volleys + Curve + FKAccuracy + LongPassing + Agility + Reactions + Balance + ShotPower + Jumping + Stamina + Strength  + Aggression + Vision + Penalties + Composure + GKDiving + GKHandling + GKKicking + GKPositioning + GKReflexes + Preferred.Foot + Work.Rate + Body.Type + Position, data = train_data)

model2 = step(temp_model_2, trace = 0)

names(coef(model2))
```

A second model (above) was created using backwards AIC search so that there were more variables to experiment with.

```{r}
par(mfrow = c(1, 2))
plot(fitted(model2), resid(model2), col = "grey")
abline(h = 0, col = "orange", lwd = 2)

qqnorm(resid(model2), col = "grey")
qqline(resid(model2), col = "dodgerblue", lwd = 2)
```

```{r}
bptest(model2)
```

```{r}
shapiro.test(resid(model2))
```
- test for loocv-rmse
```{r}
calc_loocv_rmse(model1)
```

- test for $R^2$
```{r}
summary(model1)$adj.r.squared
```

This model provided similar results to the first model. The main issue still being the violation of the constant variance assumption.

#### Model 3

```{r}
temp_model_3 = lm(Overall ~ 1, data = train_data)

model3 = step(temp_model_3, scope = Overall ~ Age + Height + Weight + Weak.Foot + Skill.Moves + Crossing + HeadingAccuracy + ShortPassing + Volleys + Curve + FKAccuracy + LongPassing + Agility + Reactions + Balance + ShotPower + Jumping + Stamina + Strength  + Aggression + Vision + Penalties + Composure + GKDiving + GKHandling + GKKicking + GKPositioning + GKReflexes + Preferred.Foot + Work.Rate + Body.Type + Position, direction = 'forward', trace = 0)

names(coef(model3))
```

Lastly, a forward AIC approach was used and the predictor variables above were selected. 

```{r}
bptest(model3)

shapiro.test(resid(model3))
```
- test for loocv-rmse
```{r}
calc_loocv_rmse(model1)
```

- test for $R^2$
```{r}
summary(model1)$adj.r.squared
```

This model violated both the normality and the constant variance assumption. Based on these three model it is clear that constant variance is the greatest issue over normality, prediction accuracy, and significance of regression.

### Variance Stabilization
#### Response Transformation

To get a sense for what kind of transformation should be required, scatter plots were created.

- plotting
```{r}
plot(Overall ~ Reactions + Composure + ShortPassing  + HeadingAccuracy + ShotPower + Skill.Moves  + Agility + Strength + Height + Vision + Stamina + Weak.Foot + Aggression + Weight + Crossing + Volleys, data = file)
```

When inspecting the graphs above, many contain curve upwards similar to the "Salaries at Initech, By Seniority" graph. Therefore, it seems that a log transformation on the response is approprate.


#### Box-Cox Transformation

Box-Cox Transformations were considered, however, they did not improve the p-value of the B-P test at all. The lambda values found were between -20 and -3 and resulted in B-P p-values of around $e^{-6}$ in magnitude. Therefore, Box-Cox transformations were disregarded.


### Results

To find a model which did not violate model assumptions, many iterations of guessing and checking with transforming and removing predictor variables were done. The final model is listed below.

```{r}
modelf5 = lm(log(Overall) ~ Weight + HeadingAccuracy + ShortPassing + Reactions + log(ShotPower) + log(Stamina) + log(Vision) + log(Composure) + Strength, data = test_data)

par(mfrow = c(1, 2))
plot(fitted(modelf5), resid(modelf5), col = "grey")
abline(h = 0, col = "orange", lwd = 2)

qqnorm(resid(modelf5), col = "grey")
qqline(resid(modelf5), col = "dodgerblue", lwd = 2)

```{r}
bptest(modelf5)
shapiro.test(resid(mod5))
```

```{r}
calc_loocv_rmse(mod5)
```

```{r}
summary(mod5)$adj.r.square
```

Based on the values above, it appears that p-values for the B-P Test and Shapiro-Wilks Test are reasonably large enough to pass the assumptions. Also, the LOOCV-RMSE and Adjacent $R^2$ values were small and large enough respectively.

It is worth noting that included "International.Reputation" increases the significance of the regression without violating any assumptions. However, the main goal of this study was to attempt to predict how good a player is perceived by some authority (FIFA). It seems that rating a player based on how they are rated by other people is almost cheating.

